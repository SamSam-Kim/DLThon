{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0203801",
   "metadata": {},
   "source": [
    "사전학습된 한국어 BERT(klue/bert-base) 모델을 사용하지만\n",
    "\n",
    "BERT 본체(encoder)는 모두 동결(freeze)하고, 분류기 헤드(classifier)만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212882d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 하이퍼파라미터\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "N_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "AUGMENT_K = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# 데이터 로드\n",
    "print(\"데이터 로드 중...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "if 'text' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'text': 'conversation'})\n",
    "if 'file_name' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'file_name': 'idx'})\n",
    "\n",
    "# 텍스트 전처리\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip())\n",
    "    return text\n",
    "train_df['conversation'] = train_df['conversation'].apply(clean_text)\n",
    "test_df['conversation'] = test_df['conversation'].apply(clean_text)\n",
    "\n",
    "# 클래스 표준화 및 매핑\n",
    "custom_classes = ['협박', '갈취', '직장내괴롭힘', '기타괴롭힘', '일반']\n",
    "class2idx = {\n",
    "    '협박': 0, '협박 대화': 0,\n",
    "    '갈취': 1, '갈취 대화': 1,\n",
    "    '직장내괴롭힘': 2, '직장 내 괴롭힘 대화': 2,\n",
    "    '기타괴롭힘': 3, '기타 괴롭힘 대화': 3,\n",
    "    '일반': 4, '일반대화': 4\n",
    "}\n",
    "def extract_class(x):\n",
    "    if '협박' in x:\n",
    "        return '협박'\n",
    "    elif '갈취' in x:\n",
    "        return '갈취'\n",
    "    elif '직장' in x:\n",
    "        return '직장내괴롭힘'\n",
    "    elif '기타' in x:\n",
    "        return '기타괴롭힘'\n",
    "    elif '일반' in x:\n",
    "        return '일반'\n",
    "    return x\n",
    "train_df['class'] = train_df['class'].apply(extract_class)\n",
    "train_df['target'] = train_df['class'].map(class2idx)\n",
    "assert train_df['target'].isna().sum() == 0, \"클래스 매핑 오류 발생!\"\n",
    "\n",
    "idx2class = {i: c for i, c in enumerate(custom_classes)}\n",
    "\n",
    "# 토크나이저\n",
    "print(f\"토크나이저 로드 중: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(texts, labels, k=AUGMENT_K):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        augmented_texts.append(text)\n",
    "        augmented_labels.append(label)\n",
    "        if k > 0:\n",
    "            sentences = text.split('. ')\n",
    "            if len(sentences) > 1:\n",
    "                shuffled = '. '.join(sentences[::-1])\n",
    "                augmented_texts.append(shuffled)\n",
    "                augmented_labels.append(label)\n",
    "            if len(sentences) > 2:\n",
    "                del_idx = random.randint(0, len(sentences)-1)\n",
    "                new_text = '. '.join(sentences[:del_idx] + sentences[del_idx+1:])\n",
    "                augmented_texts.append(new_text)\n",
    "                augmented_labels.append(label)\n",
    "            words = text.split()\n",
    "            if len(words) > 5:\n",
    "                for _ in range(min(k-2, 1)):\n",
    "                    mod_text = text\n",
    "                    if '은' in mod_text:\n",
    "                        mod_text = mod_text.replace('은', '는', 1)\n",
    "                    elif '는' in mod_text:\n",
    "                        mod_text = mod_text.replace('는', '은', 1)\n",
    "                    elif '이' in mod_text:\n",
    "                        mod_text = mod_text.replace('이', '가', 1)\n",
    "                    elif '가' in mod_text:\n",
    "                        mod_text = mod_text.replace('가', '이', 1)\n",
    "                    if mod_text != text:\n",
    "                        augmented_texts.append(mod_text)\n",
    "                        augmented_labels.append(label)\n",
    "    return augmented_texts, augmented_labels\n",
    "\n",
    "# 일반대화 합성 데이터 예시\n",
    "def create_normal_conversations(n=1000):\n",
    "    print(\"일반 대화 데이터 생성 중...\")\n",
    "    normal_conversations = [\n",
    "        \"이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 요즘 듣는 것도 들어봐 옴 난 좀 별로네 아님 넌 취향은 아닌 듯 배고프다 밥이나 먹으러 가자 그래\"\n",
    "    ]\n",
    "    return normal_conversations * (n // len(normal_conversations))\n",
    "\n",
    "normal_convs = create_normal_conversations()\n",
    "normal_df = pd.DataFrame({\n",
    "    'class': ['일반'] * len(normal_convs),\n",
    "    'conversation': normal_convs,\n",
    "    'target': [4] * len(normal_convs)\n",
    "})\n",
    "train_df = pd.concat([train_df, normal_df], ignore_index=True)\n",
    "\n",
    "# PyTorch Dataset\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 교차 검증\n",
    "print(f\"{N_FOLDS}개 폴드 교차 검증 시작...\")\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "test_predictions_folds = []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['conversation'], train_df['target'])):\n",
    "    print(f\"\\n===== 폴드 {fold+1}/{N_FOLDS} 학습 시작 =====\")\n",
    "    X_train = train_df['conversation'].iloc[train_idx].tolist()\n",
    "    y_train = train_df['target'].iloc[train_idx].values\n",
    "    X_val = train_df['conversation'].iloc[val_idx].tolist()\n",
    "    y_val = train_df['target'].iloc[val_idx].values\n",
    "\n",
    "    # 데이터 증강\n",
    "    print(\"데이터 증강 적용 중...\")\n",
    "    augmented_texts, augmented_labels = augment_data(X_train, y_train)\n",
    "    print(f\"증강 전: {len(X_train)}개, 증강 후: {len(augmented_texts)}개\")\n",
    "    train_dataset = ConversationDataset(augmented_texts, augmented_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = ConversationDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 모델 로드\n",
    "    print(f\"모델 로드 중: {MODEL_NAME}\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(custom_classes)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 학습\n",
    "    print(f\"모델 학습 중...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # 검증 세트 예측\n",
    "    print(\"검증 세트 평가 중...\")\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "    val_f1 = f1_score(y_val, val_preds, average='macro')\n",
    "    print(f\"검증 F1 점수: {val_f1:.4f}\")\n",
    "\n",
    "    # 테스트 세트 예측\n",
    "    print(\"테스트 세트 예측 중...\")\n",
    "    test_texts = test_df['conversation'].tolist()\n",
    "    test_dataset = ConversationDataset(test_texts, labels=None, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            test_preds.extend(preds)\n",
    "    test_predictions_folds.append(test_preds)\n",
    "    fold_metrics.append({'fold': fold + 1, 'val_f1': val_f1})\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 앙상블 (hard voting)\n",
    "test_predictions = np.zeros((len(test_df),), dtype=int)\n",
    "test_pred_array = np.array(test_predictions_folds)\n",
    "for i in range(len(test_df)):\n",
    "    test_predictions[i] = np.bincount(test_pred_array[:, i]).argmax()\n",
    "\n",
    "# 예측 분포 확인\n",
    "pred_dist = {idx2class[i]: np.sum(test_predictions == i) for i in range(len(custom_classes))}\n",
    "print(\"\\n예측 클래스 분포:\")\n",
    "for cls, count in pred_dist.items():\n",
    "    print(f\"{cls}: {count} ({count/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "# 결과 저장\n",
    "submission = pd.DataFrame({\n",
    "    'idx': test_df['idx'],\n",
    "    'target': test_predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n제출 파일 생성 완료: submission.csv\")\n",
    "\n",
    "mean_f1 = np.mean([m['val_f1'] for m in fold_metrics])\n",
    "print(f\"\\n교차검증 평균 F1 점수: {mean_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
