{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbfde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re\n",
    "\n",
    "# GPU 메모리 최적화\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(f\"{len(tf.config.list_physical_devices('GPU'))} Physical GPUs, memory growth enabled\")\n",
    "\n",
    "# 1. 데이터 불러오기 및 컬럼명 정리\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df = test_df.rename(columns={'text': 'conversation', 'idx': 'file_name'})\n",
    "\n",
    "# 2. 텍스트 전처리 및 클래스명 통일\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip())\n",
    "    return text\n",
    "\n",
    "def clean_class(x):\n",
    "    return x.replace(' ', '').replace('대화', '')\n",
    "\n",
    "train_df['conversation'] = train_df['conversation'].apply(clean_text)\n",
    "test_df['conversation'] = test_df['conversation'].apply(clean_text)\n",
    "train_df['class'] = train_df['class'].apply(clean_class)\n",
    "\n",
    "# 3. 클래스 매핑\n",
    "custom_classes = ['협박', '갈취', '직장내괴롭힘', '기타괴롭힘', '일반']\n",
    "class2idx = {c: i for i, c in enumerate(custom_classes)}\n",
    "idx2class = {i: c for i, c in enumerate(custom_classes)}\n",
    "train_df['target'] = train_df['class'].map(class2idx)\n",
    "\n",
    "# 4. 특수 토큰 추가\n",
    "train_df['conversation'] = \"<start> \" + train_df['conversation'].astype(str) + \" <extract>\"\n",
    "test_df['conversation'] = \"<start> \" + test_df['conversation'].astype(str) + \" <extract>\"\n",
    "\n",
    "# 5. 데이터 분리\n",
    "X = train_df['conversation'].values\n",
    "y = train_df['target'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 6. 텍스트 벡터화\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 128\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN,\n",
    "    standardize=None\n",
    ")\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "# 7. Positional Embedding\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = Embedding(VOCAB_SIZE, embed_dim)\n",
    "        self.pos_emb = Embedding(max_len, embed_dim)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        pos_embeddings = self.pos_emb(positions)\n",
    "        tok_embeddings = self.token_emb(x)\n",
    "        return tok_embeddings + pos_embeddings\n",
    "\n",
    "# 8. GPT-1 Block \n",
    "class GPTBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='gelu'),\n",
    "            Dropout(rate),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
    "        attn_output = self.att(x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# 9. GPT-1 분류 모델 정의 \n",
    "def create_gpt1_classifier(vocab_size, max_len, embed_dim, num_heads, ff_dim, num_layers, num_classes):\n",
    "    inputs = Input(shape=(1,), dtype=tf.string)\n",
    "    x = vectorize_layer(inputs)\n",
    "    x = PositionalEmbedding(max_len, embed_dim)(x)\n",
    "    for _ in range(num_layers):\n",
    "        x = GPTBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 10. 클래스 가중치\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# 11. 하이퍼파라미터 (메모리 절약형)\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 512\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 학습률 스케줄링\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate=2e-4,\n",
    "    first_decay_steps=1000,\n",
    "    t_mul=2.0,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "model = create_gpt1_classifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=len(custom_classes)\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor(X_train.reshape(-1, 1))\n",
    "X_val_tensor = tf.convert_to_tensor(X_val.reshape(-1, 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_tensor,\n",
    "    y_train,\n",
    "    validation_data=(X_val_tensor, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 검증 데이터 F1 Score\n",
    "print(\"\\n[Validation F1 Score 계산]\")\n",
    "y_val_pred = model.predict(X_val_tensor).argmax(axis=1)\n",
    "val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "print(f\"Validation F1 Score (Macro): {val_f1:.4f}\")\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "print(\"\\n[테스트 데이터 예측]\")\n",
    "X_test = test_df['conversation'].values\n",
    "X_test_tensor = tf.convert_to_tensor(X_test.reshape(-1, 1))\n",
    "y_pred = model.predict(X_test_tensor).argmax(axis=1)\n",
    "\n",
    "# 예측 결과 분포 확인\n",
    "from collections import Counter\n",
    "print(\"\\n예측 클래스 분포:\")\n",
    "class_names = [idx2class[i] for i in y_pred]\n",
    "print(Counter(class_names))\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission_3 = pd.DataFrame({\n",
    "    'idx': test_df['file_name'],\n",
    "    'target': y_pred\n",
    "})\n",
    "submission_3.to_csv('submission_3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
