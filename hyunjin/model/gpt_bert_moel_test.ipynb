{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece7b129",
   "metadata": {},
   "source": [
    "KLUE-RoBERTa-large + 데이터 증강 + 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf28f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 하이퍼파라미터\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "N_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "AUGMENT_K = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# 데이터 로드\n",
    "print(\"데이터 로드 중...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "if 'text' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'text': 'conversation'})\n",
    "if 'file_name' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'file_name': 'idx'})\n",
    "\n",
    "# 텍스트 전처리\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text).strip())\n",
    "    return text\n",
    "train_df['conversation'] = train_df['conversation'].apply(clean_text)\n",
    "test_df['conversation'] = test_df['conversation'].apply(clean_text)\n",
    "\n",
    "# 클래스 표준화 및 매핑\n",
    "custom_classes = ['협박', '갈취', '직장내괴롭힘', '기타괴롭힘', '일반']\n",
    "class2idx = {\n",
    "    '협박': 0, '협박 대화': 0,\n",
    "    '갈취': 1, '갈취 대화': 1,\n",
    "    '직장내괴롭힘': 2, '직장 내 괴롭힘 대화': 2,\n",
    "    '기타괴롭힘': 3, '기타 괴롭힘 대화': 3,\n",
    "    '일반': 4, '일반대화': 4\n",
    "}\n",
    "def extract_class(x):\n",
    "    if '협박' in x:\n",
    "        return '협박'\n",
    "    elif '갈취' in x:\n",
    "        return '갈취'\n",
    "    elif '직장' in x:\n",
    "        return '직장내괴롭힘'\n",
    "    elif '기타' in x:\n",
    "        return '기타괴롭힘'\n",
    "    elif '일반' in x:\n",
    "        return '일반'\n",
    "    return x\n",
    "train_df['class'] = train_df['class'].apply(extract_class)\n",
    "train_df['target'] = train_df['class'].map(class2idx)\n",
    "assert train_df['target'].isna().sum() == 0, \"클래스 매핑 오류 발생!\"\n",
    "\n",
    "idx2class = {i: c for i, c in enumerate(custom_classes)}\n",
    "\n",
    "# 토크나이저\n",
    "print(f\"토크나이저 로드 중: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(texts, labels, k=AUGMENT_K):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        augmented_texts.append(text)\n",
    "        augmented_labels.append(label)\n",
    "        if k > 0:\n",
    "            sentences = text.split('. ')\n",
    "            if len(sentences) > 1:\n",
    "                shuffled = '. '.join(sentences[::-1])\n",
    "                augmented_texts.append(shuffled)\n",
    "                augmented_labels.append(label)\n",
    "            if len(sentences) > 2:\n",
    "                del_idx = random.randint(0, len(sentences)-1)\n",
    "                new_text = '. '.join(sentences[:del_idx] + sentences[del_idx+1:])\n",
    "                augmented_texts.append(new_text)\n",
    "                augmented_labels.append(label)\n",
    "            words = text.split()\n",
    "            if len(words) > 5:\n",
    "                for _ in range(min(k-2, 1)):\n",
    "                    mod_text = text\n",
    "                    if '은' in mod_text:\n",
    "                        mod_text = mod_text.replace('은', '는', 1)\n",
    "                    elif '는' in mod_text:\n",
    "                        mod_text = mod_text.replace('는', '은', 1)\n",
    "                    elif '이' in mod_text:\n",
    "                        mod_text = mod_text.replace('이', '가', 1)\n",
    "                    elif '가' in mod_text:\n",
    "                        mod_text = mod_text.replace('가', '이', 1)\n",
    "                    if mod_text != text:\n",
    "                        augmented_texts.append(mod_text)\n",
    "                        augmented_labels.append(label)\n",
    "    return augmented_texts, augmented_labels\n",
    "\n",
    "# 일반대화 합성 데이터 예시\n",
    "def create_normal_conversations(n=1000):\n",
    "    print(\"일반 대화 데이터 생성 중...\")\n",
    "    normal_conversations = [\n",
    "        \"이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 요즘 듣는 것도 들어봐 옴 난 좀 별로네 아님 넌 취향은 아닌 듯 배고프다 밥이나 먹으러 가자 그래\"\n",
    "    ]\n",
    "    return normal_conversations * (n // len(normal_conversations))\n",
    "\n",
    "normal_convs = create_normal_conversations()\n",
    "normal_df = pd.DataFrame({\n",
    "    'class': ['일반'] * len(normal_convs),\n",
    "    'conversation': normal_convs,\n",
    "    'target': [4] * len(normal_convs)\n",
    "})\n",
    "train_df = pd.concat([train_df, normal_df], ignore_index=True)\n",
    "\n",
    "# PyTorch Dataset\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 교차 검증\n",
    "print(f\"{N_FOLDS}개 폴드 교차 검증 시작...\")\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "test_predictions_folds = []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['conversation'], train_df['target'])):\n",
    "    print(f\"\\n===== 폴드 {fold+1}/{N_FOLDS} 학습 시작 =====\")\n",
    "    X_train = train_df['conversation'].iloc[train_idx].tolist()\n",
    "    y_train = train_df['target'].iloc[train_idx].values\n",
    "    X_val = train_df['conversation'].iloc[val_idx].tolist()\n",
    "    y_val = train_df['target'].iloc[val_idx].values\n",
    "\n",
    "    # 데이터 증강\n",
    "    print(\"데이터 증강 적용 중...\")\n",
    "    augmented_texts, augmented_labels = augment_data(X_train, y_train)\n",
    "    print(f\"증강 전: {len(X_train)}개, 증강 후: {len(augmented_texts)}개\")\n",
    "    train_dataset = ConversationDataset(augmented_texts, augmented_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = ConversationDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 모델 로드\n",
    "    print(f\"모델 로드 중: {MODEL_NAME}\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(custom_classes)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 학습\n",
    "    print(f\"모델 학습 중...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # 검증 세트 예측\n",
    "    print(\"검증 세트 평가 중...\")\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "    val_f1 = f1_score(y_val, val_preds, average='macro')\n",
    "    print(f\"검증 F1 점수: {val_f1:.4f}\")\n",
    "\n",
    "    # 테스트 세트 예측\n",
    "    print(\"테스트 세트 예측 중...\")\n",
    "    test_texts = test_df['conversation'].tolist()\n",
    "    test_dataset = ConversationDataset(test_texts, labels=None, tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            test_preds.extend(preds)\n",
    "    test_predictions_folds.append(test_preds)\n",
    "    fold_metrics.append({'fold': fold + 1, 'val_f1': val_f1})\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 앙상블 (hard voting)\n",
    "test_predictions = np.zeros((len(test_df),), dtype=int)\n",
    "test_pred_array = np.array(test_predictions_folds)\n",
    "for i in range(len(test_df)):\n",
    "    test_predictions[i] = np.bincount(test_pred_array[:, i]).argmax()\n",
    "\n",
    "# 예측 분포 확인\n",
    "pred_dist = {idx2class[i]: np.sum(test_predictions == i) for i in range(len(custom_classes))}\n",
    "print(\"\\n예측 클래스 분포:\")\n",
    "for cls, count in pred_dist.items():\n",
    "    print(f\"{cls}: {count} ({count/len(test_predictions)*100:.2f}%)\")\n",
    "\n",
    "# 결과 저장\n",
    "submission = pd.DataFrame({\n",
    "    'idx': test_df['idx'],\n",
    "    'target': test_predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n제출 파일 생성 완료: submission.csv\")\n",
    "\n",
    "mean_f1 = np.mean([m['val_f1'] for m in fold_metrics])\n",
    "print(f\"\\n교차검증 평균 F1 점수: {mean_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT-GPT 하이브리드 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23874ab6",
   "metadata": {},
   "source": [
    "v.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ca17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, GPT2Model\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터셋 클래스\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 2. 하이브리드 모델\n",
    "class BertGptClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, bert_model='bert-base-uncased', gpt_model='gpt2'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.gpt = GPT2Model.from_pretrained(gpt_model)\n",
    "        \n",
    "        # BERT-GPT 특징 융합 레이어\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768 + 768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT 특징 추출\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        bert_features = bert_output.last_hidden_state[:, 0, :]  # [CLS] 토큰\n",
    "        \n",
    "        # GPT 특징 추출\n",
    "        gpt_output = self.gpt(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        gpt_features = gpt_output.last_hidden_state[:, -1, :]  # 마지막 토큰\n",
    "        \n",
    "        # 특징 융합\n",
    "        combined = torch.cat([bert_features, gpt_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # 분류\n",
    "        return self.classifier(fused)\n",
    "\n",
    "# 3. 학습 함수 (F1 Score 계산 포함)\n",
    "def train_model(model, dataloader, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 예측값 저장\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return total_loss / len(dataloader), f1\n",
    "\n",
    "# 4. 평가 함수\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return total_loss / len(dataloader), f1\n",
    "\n",
    "# 5. 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 하이퍼파라미터\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-5\n",
    "    MAX_LEN = 128\n",
    "    \n",
    "    # 데이터 준비 (예시)\n",
    "    train_texts = [\"text1\", \"text2\", ...]  # 실제 텍스트 데이터\n",
    "    train_labels = [0, 1, ...]            # 실제 레이블\n",
    "    val_texts = [\"text3\", \"text4\", ...]\n",
    "    val_labels = [1, 0, ...]\n",
    "    \n",
    "    # 토크나이저 (BERT/GPT 호환)\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    \n",
    "    # 데이터로더\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # 장치 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = BertGptClassifier(num_classes=5)  # 클래스 수 수정\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 옵티마이저\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # 학습 루프\n",
    "    best_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_f1 = train_model(model, train_loader, optimizer, device)\n",
    "        val_loss, val_f1 = eval_model(model, val_loader, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f'Best Validation F1: {best_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de04c1",
   "metadata": {},
   "source": [
    "v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, GPT2Model\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터셋 클래스\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 2. 하이브리드 모델\n",
    "class BertGptClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, bert_model='bert-base-uncased', gpt_model='gpt2'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.gpt = GPT2Model.from_pretrained(gpt_model)\n",
    "        \n",
    "        # BERT-GPT 특징 융합 레이어\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768 + 768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT 특징 추출\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        bert_features = bert_output.last_hidden_state[:, 0, :]  # [CLS] 토큰\n",
    "        \n",
    "        # GPT 특징 추출\n",
    "        gpt_output = self.gpt(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        gpt_features = gpt_output.last_hidden_state[:, -1, :]  # 마지막 토큰\n",
    "        \n",
    "        # 특징 융합\n",
    "        combined = torch.cat([bert_features, gpt_features], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # 분류\n",
    "        return self.classifier(fused)\n",
    "\n",
    "# 3. 학습 함수 (F1 Score 계산 포함)\n",
    "def train_model(model, dataloader, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 예측값 저장\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return total_loss / len(dataloader), f1\n",
    "\n",
    "# 4. 평가 함수\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return total_loss / len(dataloader), f1\n",
    "\n",
    "# 5. 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 하이퍼파라미터\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-5\n",
    "    MAX_LEN = 128\n",
    "    \n",
    "    # 데이터 준비 (예시)\n",
    "    train_texts = [\"text1\", \"text2\", ...]  # 실제 텍스트 데이터\n",
    "    train_labels = [0, 1, ...]            # 실제 레이블\n",
    "    val_texts = [\"text3\", \"text4\", ...]\n",
    "    val_labels = [1, 0, ...]\n",
    "    \n",
    "    # 토크나이저 (BERT/GPT 호환)\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    \n",
    "    # 데이터로더\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # 장치 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = BertGptClassifier(num_classes=5)  # 클래스 수 수정\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 옵티마이저\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # 학습 루프\n",
    "    best_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_f1 = train_model(model, train_loader, optimizer, device)\n",
    "        val_loss, val_f1 = eval_model(model, val_loader, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f'Best Validation F1: {best_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5beb43",
   "metadata": {},
   "source": [
    "colab 경량화 버전(T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필수 라이브러리 설치 및 환경설정\n",
    "!pip install -U transformers==4.41.0 sentence-transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 2. Colab 파일 업로드 (train.csv, test.csv)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 3. 데이터 로드 및 전처리\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?가-힣]', '', str(text).strip())  # 한글/영문/숫자/기본 부호만\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text[:512]\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "if 'text' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'text': 'conversation'})\n",
    "if 'file_name' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'file_name': 'idx'})\n",
    "\n",
    "train_df['conversation'] = train_df['conversation'].apply(clean_text)\n",
    "test_df['conversation'] = test_df['conversation'].apply(clean_text)\n",
    "\n",
    "custom_classes = ['협박', '갈취', '직장내괴롭힘', '기타괴롭힘', '일반']\n",
    "class2idx = {\n",
    "    '협박': 0, '협박 대화': 0,\n",
    "    '갈취': 1, '갈취 대화': 1,\n",
    "    '직장내괴롭힘': 2, '직장 내 괴롭힘 대화': 2,\n",
    "    '기타괴롭힘': 3, '기타 괴롭힘 대화': 3,\n",
    "    '일반': 4, '일반대화': 4\n",
    "}\n",
    "def extract_class(x):\n",
    "    if '협박' in x:\n",
    "        return '협박'\n",
    "    elif '갈취' in x:\n",
    "        return '갈취'\n",
    "    elif '직장' in x:\n",
    "        return '직장내괴롭힘'\n",
    "    elif '기타' in x:\n",
    "        return '기타괴롭힘'\n",
    "    elif '일반' in x:\n",
    "        return '일반'\n",
    "    return x\n",
    "train_df['class'] = train_df['class'].apply(extract_class)\n",
    "train_df['target'] = train_df['class'].map(class2idx)\n",
    "assert train_df['target'].isna().sum() == 0, \"클래스 매핑 오류 발생!\"\n",
    "\n",
    "idx2class = {i: c for i, c in enumerate(custom_classes)}\n",
    "\n",
    "# 클래스 분포 시각화\n",
    "plt.figure(figsize=(8,4))\n",
    "train_df['class'].value_counts().plot(kind='bar')\n",
    "plt.title('Train Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# 4. 토크나이저 준비 (한국어면 'klue/bert-base')\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 5. 데이터셋 정의\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128, mlm_prob=0.15):\n",
    "        self.encodings = tokenizer(texts.tolist(), padding='max_length', truncation=True, max_length=max_len)\n",
    "        self.labels = labels.tolist() if labels is not None else None\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm_prob = mlm_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encodings['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encodings['attention_mask'][idx])\n",
    "\n",
    "        # MLM 마스킹\n",
    "        mlm_labels = input_ids.clone()\n",
    "        probability_matrix = torch.full(input_ids.shape, self.mlm_prob)\n",
    "        special_tokens_mask = self.tokenizer.get_special_tokens_mask(input_ids.tolist(), already_has_special_tokens=True)\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        input_ids_masked = input_ids.clone()\n",
    "        input_ids_masked[masked_indices] = self.tokenizer.mask_token_id\n",
    "        mlm_labels[~masked_indices] = -100\n",
    "\n",
    "        # CLM 레이블 (shift)\n",
    "        clm_labels = input_ids.clone()\n",
    "        clm_labels[0] = -100\n",
    "\n",
    "        item = {\n",
    "            'input_ids': input_ids_masked,\n",
    "            'attention_mask': attention_mask,\n",
    "            'mlm_labels': mlm_labels,\n",
    "            'clm_labels': clm_labels\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# 6. 학습/검증 데이터 분할\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['conversation'], train_df['target'], test_size=0.2, stratify=train_df['target'], random_state=42\n",
    ")\n",
    "train_dataset = HybridDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = HybridDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# 7. 경량화 하이브리드 트랜스포머 모델 정의\n",
    "class HybridTransformerForMNTP(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=512, num_layers=6, num_heads=8, num_classes=5):\n",
    "        super().__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_hidden_layers=num_layers,\n",
    "            is_decoder=True,\n",
    "            add_cross_attention=False\n",
    "        )\n",
    "        self.transformer = BertModel(config)\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(outputs.last_hidden_state)\n",
    "        # 분류 헤드: [CLS] 토큰만\n",
    "        cls_logits = self.classifier(outputs.last_hidden_state[:,0,:])\n",
    "        return logits, cls_logits\n",
    "\n",
    "# 8. 손실 함수\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlm_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.clm_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    def forward(self, logits, mlm_labels, clm_labels, cls_logits, labels):\n",
    "        mlm_loss = self.mlm_loss(logits.view(-1, logits.size(-1)), mlm_labels.view(-1))\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = clm_labels[:, 1:].contiguous()\n",
    "        clm_loss = self.clm_loss(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        ce_loss = self.ce_loss(cls_logits, labels)\n",
    "        return 0.5*mlm_loss + 0.2*clm_loss + 0.3*ce_loss\n",
    "\n",
    "# 9. 학습 준비\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridTransformerForMNTP(vocab_size=tokenizer.vocab_size).to(device)\n",
    "criterion = HybridLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# 10. 평가 및 분석 함수\n",
    "def evaluate_and_analyze(model, loader, device, tokenizer, epoch):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    misclassified_samples = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                _, cls_logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(cls_logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            # 오분류 샘플 수집\n",
    "            for i in range(len(preds)):\n",
    "                if preds[i] != labels[i] and len(misclassified_samples) < 5:\n",
    "                    orig_text = tokenizer.decode([x for x in input_ids[i].cpu().tolist() if x != tokenizer.pad_token_id], skip_special_tokens=True)\n",
    "                    misclassified_samples.append({\n",
    "                        'text': orig_text,\n",
    "                        'pred': idx2class[preds[i].item()],\n",
    "                        'true': idx2class[labels[i].item()]\n",
    "                    })\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"\\nEpoch {epoch+1} F1: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=custom_classes, zero_division=0))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    pd.Series(all_preds).value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title(\"Predicted Class Distribution\")\n",
    "    plt.xticks(ticks=range(len(custom_classes)), labels=custom_classes, rotation=45)\n",
    "    plt.show()\n",
    "    print(\"\\nTop 5 Misclassified Samples:\")\n",
    "    for sample in misclassified_samples:\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(f\"Predicted: {sample['pred']} | True: {sample['true']}\\n\")\n",
    "\n",
    "# 11. 학습 루프\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        mlm_labels = batch['mlm_labels'].to(device)\n",
    "        clm_labels = batch['clm_labels'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits, cls_logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, mlm_labels, clm_labels, cls_logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    evaluate_and_analyze(model, val_loader, device, tokenizer, epoch)\n",
    "\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "# 12. 테스트 예측 및 CSV 저장\n",
    "test_dataset = HybridDataset(test_df['conversation'], [0]*len(test_df), tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            _, cls_logits = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(cls_logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'idx': test_df['idx'],\n",
    "    'target': test_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"제출 파일 생성 완료: submission.csv\")\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
