{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ffcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalMaxPool1D, LayerNormalization, Dropout, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0. 특수 토큰 추가 (변경)\n",
    "SPECIAL_TOKENS = [\"[CLS]\", \"[SEP]\", \"[PAD]\"]  # 분류 작업용 특수 토큰\n",
    "\n",
    "# 1. 데이터 준비 (변경: 특수 토큰 추가 전처리)\n",
    "df = pd.read_csv('train_merged.csv')\n",
    "df = df.rename(columns={'class': 'target'})\n",
    "\n",
    "# 대화 데이터에 [CLS] 토큰 추가 (예시)\n",
    "df['conversation'] = \"[CLS] \" + df['conversation'].astype(str) + \" [SEP]\"\n",
    "\n",
    "# 레이블 인코딩\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['target'])\n",
    "\n",
    "# 데이터 분할\n",
    "X = df['conversation'].values  # 변경: 이미 문자열로 처리됨\n",
    "y = df['target'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 2. 텍스트 벡터화 (변경: 특수 토큰 고려)\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 특수 토큰을 포함한 어휘 사전 생성\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN,\n",
    "    standardize=None,  # 변경: 기본 전처리 비활성화\n",
    "    name='text_vectorization'\n",
    ")\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "# 3. Positional Embedding 레이어 추가 (신규)\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = Embedding(input_dim=VOCAB_SIZE, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(start=0, limit=tf.shape(inputs)[-1], delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n",
    "\n",
    "# 4. 트랜스포머 블록 변경 (GELU 적용)\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='gelu'),  # 변경: ReLU → GELU\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# 5. 모델 구성 함수 변경 (PositionalEmbedding 추가)\n",
    "def create_gpt1_model(vocab_size, max_len, embed_dim, num_heads, ff_dim, num_layers, num_classes):\n",
    "    inputs = Input(shape=(1,), dtype=tf.string)\n",
    "    x = vectorize_layer(inputs)\n",
    "    x = PositionalEmbedding(max_len, embed_dim)(x)  # 변경: 기존 Embedding → PositionalEmbedding\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', \n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 6. 사전학습을 위한 언어 모델 구성 (신규)\n",
    "def create_pretrain_model(base_model):\n",
    "    inputs = Input(shape=(1,), dtype=tf.string)\n",
    "    x = base_model(inputs)\n",
    "    outputs = Dense(VOCAB_SIZE, activation='softmax')(x)  # 다음 단어 예측\n",
    "    pretrain_model = Model(inputs, outputs)\n",
    "    pretrain_model.compile(optimizer='adam', \n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "    return pretrain_model\n",
    "\n",
    "# 7. 학습 파이프라인 변경\n",
    "base_model = create_gpt1_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_len=MAX_LEN,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    ff_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=len(le.classes_)\n",
    ")\n",
    "\n",
    "# 사전학습 단계 (예시, 실제로는 대규모 데이터 필요)\n",
    "pretrain_model = create_pretrain_model(base_model)\n",
    "# pretrain_model.fit(large_unlabeled_data, ...)  # 실제 구현시 활성화\n",
    "\n",
    "# 미세조정 단계\n",
    "history = base_model.fit(\n",
    "    tf.convert_to_tensor(X_train),\n",
    "    tf.convert_to_tensor(y_train),\n",
    "    validation_data=(tf.convert_to_tensor(X_val), tf.convert_to_tensor(y_val)),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[F1Callback(X_val, y_val)]  \n",
    ")\n",
    "\n",
    "# 7. 학습 곡선 시각화\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(f1_callback.val_f1, label='F1')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
